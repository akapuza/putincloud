d2 <- dist(as.matrix(dm2))
hc <- hclust(d2, method="ward.D")
plot(hc)
groups <- cutree(hc, k=9)
rect.hclust(hc, k=9, border="red")
docs2 <- tm_map(docs2, removeWords, "или", "нам", "такж", "сво")
docs2 <- tm_map(docs2, stemDocument, language="russian")
#Text to term matrix
dtm2 <- TermDocumentMatrix(docs2)
m2 <- as.matrix(dtm2)
v2 <- sort(rowSums(m2),decreasing=TRUE)
d2 <- data.frame(word = names(v2),freq=v2)
head(d2, 23)
View(d2)
# Associations and cluster analysis
findAssocs(dtm, c("реш", "регион", "наш", "эпид", "ситуац",
"здор", "самоизоляц"), 0.5)
# Associations and cluster analysis
findAssocs(dtm2, c("реш", "регион", "наш", "эпид", "ситуац",
"здор", "самоизоляц"), 0.5)
word_associate(text, match.string = "самоизоляц",
stopwords = sw, proportional = TRUE,
network.plot = TRUE)
sw <- c("и","в","во","не","что","он","на","я","с","со","как","а","то","все","она","так","его","но","да","ты","к","у","же","вы","за","бы","по","только","ее","мне","было","вот","от","меня","еще","нет","о","из","ему","теперь","когда","даже","ну","вдруг","ли","если","уже","или","ни","быть","был","него","до","вас","нибудь","опять","уж","вам","сказал","ведь","там","потом","себя","ничего","ей","может","они","тут","где","есть","надо","ней","для","мы","тебя","их","чем","была","сам","чтоб","без","будто","человек","чего","раз","тоже","себе","под","жизнь","будет","ж","тогда","кто","этот","говорил","того","потому","этого","какой","совсем","ним","здесь","этом","один","почти","мой","тем","чтобы","нее","кажется","сейчас","были","куда","зачем","сказать","всех","никогда","сегодня","можно","при","наконец","два","об","другой","хоть","после","над","больше","тот","через","эти","нас","про","всего","них","какая","много","разве","сказала","три","эту","моя","впрочем","хорошо","свою","этой","перед","иногда","лучше","чуть","том","нельзя","такой","им","более","всегда","конечно","всю","между")
word_associate(text2, match.string = "самоизоляц",
stopwords = sw, proportional = TRUE,
network.plot = TRUE)
word_associate(text2, match.string = "самоизоляц",
stopwords = sw, proportional = TRUE,
network.plot = TRUE)
#Read data
text2 <- readLines("putin0204.txt", encoding = "UTF-8")
docs2 <- Corpus(VectorSource(text2))
inspect(docs2)
#Prepare data
docs2 <- tm_map(docs2, stripWhitespace)
docs2 <- tm_map(docs2, content_transformer(tolower))
docs2 <- tm_map(docs2, removePunctuation)
docs2 <- tm_map(docs2, removeWords, words = stopwords("russian"))
docs2 <- tm_map(docs2, removeWords, "или", "нам", "такж", "сво")
docs2 <- tm_map(docs2, stemDocument, language="russian")
#Text to term matrix
dtm2 <- TermDocumentMatrix(docs2)
m2 <- as.matrix(dtm2)
v2 <- sort(rowSums(m2),decreasing=TRUE)
d2 <- data.frame(word = names(v2),freq=v2)
head(d2, 23)
set.seed(5)
wordcloud(words = d2$word, freq = d2$freq, min.freq=3, scale=c(3,0.2),
max.words=100, random.order=F, use.r.layout=FALSE,
colors=brewer.pal(6, "Dark2"))
set.seed(5)
wordcloud(words = d2$word, freq = d2$freq, min.freq=3, scale=c(3,0.2),
max.words=100, random.order=F, use.r.layout=FALSE,
colors=brewer.pal(6, "Dark2"))
wordcloud(words = d2$word, freq = d2$freq, min.freq=1, scale=c(2,0.2),
max.words=100, random.order=F, use.r.layout=FALSE,
colors=brewer.pal(6, "Dark2"))
wordcloud(words = d2$word, freq = d2$freq, min.freq=1, scale=c(2,0.2),
max.words=150, random.order=F, use.r.layout=FALSE,
colors=brewer.pal(6, "Dark2"))
wordcloud(words = d2$word, freq = d2$freq, min.freq=1, scale=c(2,0.2),
max.words=150, random.order=F, use.r.layout=FALSE,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d2$word, freq = d2$freq, min.freq=1, scale=c(2,0.2),
max.words=150, random.order=F, use.r.layout=FALSE,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d2$word, freq = d2$freq, min.freq=2, scale=c(2,0.2),
max.words=150, random.order=F, use.r.layout=FALSE,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d2$word, freq = d2$freq, min.freq=2, scale=c(2.5,0.1),
max.words=150, random.order=F, use.r.layout=FALSE,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d2$word, freq = d2$freq, min.freq=2, scale=c(2.5,0.1),
max.words=150, random.order=F, use.r.layout=FALSE,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d2$word, freq = d2$freq, min.freq=2, scale=c(2.5,0.1),
max.words=150, random.order=F, use.r.layout=FALSE,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d2$word, freq = d2$freq, min.freq=2, scale=c(2.3,0.1),
max.words=150, random.order=F, use.r.layout=FALSE,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d2$word, freq = d2$freq, min.freq=2, scale=c(2.2,0.1),
max.words=150, random.order=F, use.r.layout=FALSE,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d2$word, freq = d2$freq, min.freq=2, scale=c(2.2,0.1),
max.words=150, random.order=F, use.r.layout=FALSE,
colors=brewer.pal(8, "Dark2"))
# WordCloud
set.seed(4)
wordcloud(words = d2$word, freq = d2$freq, min.freq=2, scale=c(2.2,0.1),
max.words=150, random.order=F, use.r.layout=FALSE,
colors=brewer.pal(8, "Dark2"))
head(d2, 23)
View(d2)
#Frequency plot
ggplot(data = d2[1:23,], aes(x=fct_reorder(word,freq), y = freq, fill = as.factor(freq), col = as.factor(freq))) +
geom_bar(stat="identity") +
coord_flip() +
xlab("") +
ylab("Частота") +
theme(panel.background = element_rect(fill = "gray99"), axis.text.y = element_text(size=14),
axis.text.x = element_text(size=14), axis.title.x = element_text(size=14),
axis.title.y = element_text(size=14))
# Associations and cluster analysis
findAssocs(dtm2, c("решен", "регион", "наш", "эпидем", "ситуац",
"здор", "самоизоляц"), 0.5)
dm2 <- removeSparseTerms(dtm2, sparse=0.9)
d2 <- dist(as.matrix(dm2))
hc <- hclust(d2, method="ward.D")
plot(hc)
groups <- cutree(hc, k=9)
rect.hclust(hc, k=9, border="red")
# Associations and cluster analysis
findAssocs(dtm2, c("решен", "регион", "наш", "эпидем", "ситуац",
"здор", "самоизоляц", "мер"), 0.5)
sw <- c("и","в","во","не","что","он","на","я","с","со","как","а","то","все","она","так","его","но","да","ты","к","у","же","вы","за","бы","по","только","ее","мне","было","вот","от","меня","еще","нет","о","из","ему","теперь","когда","даже","ну","вдруг","ли","если","уже","или","ни","быть","был","него","до","вас","нибудь","опять","уж","вам","сказал","ведь","там","потом","себя","ничего","ей","может","они","тут","где","есть","надо","ней","для","мы","тебя","их","чем","была","сам","чтоб","без","будто","человек","чего","раз","тоже","себе","под","жизнь","будет","ж","тогда","кто","этот","говорил","того","потому","этого","какой","совсем","ним","здесь","этом","один","почти","мой","тем","чтобы","нее","кажется","сейчас","были","куда","зачем","сказать","всех","никогда","сегодня","можно","при","наконец","два","об","другой","хоть","после","над","больше","тот","через","эти","нас","про","всего","них","какая","много","разве","сказала","три","эту","моя","впрочем","хорошо","свою","этой","перед","иногда","лучше","чуть","том","нельзя","такой","им","более","всегда","конечно","всю","между")
word_associate(text2, match.string = "самоизоляц",
stopwords = sw, proportional = TRUE,
network.plot = TRUE)
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = TRUE,
network.plot = TRUE)
word_associate(text2, match.string = "самоизоляц",
stopwords = sw, proportional = TRUE,
network.plot = TRUE)
word_associate(text2, match.string = "эпидем",
stopwords = sw, proportional = TRUE,
network.plot = TRUE)
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = F,
network.plot = TRUE)
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = T,
network.plot = TRUE, wordcloud = T)
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = T,  nw.label.cex = 0.8,
network.plot = TRUE)
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = T,  nw.label.cex = 1,
network.plot = TRUE)
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = T,  nw.label.cex = 2,
network.plot = TRUE)
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = F,  nw.label.cex = 2,
network.plot = TRUE)
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = F,  nw.label.cex = 2,
nw.edge.curved = F,
network.plot = TRUE)
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = F,  nw.label.cex = 3,
network.plot = TRUE)
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = F,  nw.label.cex = 5,
network.plot = TRUE)
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = T,  nw.label.cex = 5,
network.plot = TRUE)
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = T,  nw.label.cex = 5,
network.plot = TRUE)
#Word network
sw <- c("и","в","во","не","что","он","на","я","ещё", "с","со","как","а","то","все","она","так","его","но","да","ты","к","у","же","вы","за","бы","по","только","ее","мне","было","вот","от","меня","еще","нет","о","из","ему","теперь","когда","даже","ну","вдруг","ли","если","уже","или","ни","быть","был","него","до","вас","нибудь","опять","уж","вам","сказал","ведь","там","потом","себя","ничего","ей","может","они","тут","где","есть","надо","ней","для","мы","тебя","их","чем","была","сам","чтоб","без","будто","человек","чего","раз","тоже","себе","под","жизнь","будет","ж","тогда","кто","этот","говорил","того","потому","этого","какой","совсем","ним","здесь","этом","один","почти","мой","тем","чтобы","нее","кажется","сейчас","были","куда","зачем","сказать","всех","никогда","сегодня","можно","при","наконец","два","об","другой","хоть","после","над","больше","тот","через","эти","нас","про","всего","них","какая","много","разве","сказала","три","эту","моя","впрочем","хорошо","свою","этой","перед","иногда","лучше","чуть","том","нельзя","такой","им","более","всегда","конечно","всю","между")
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = T,  nw.label.cex = 5,
network.plot = TRUE)
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = T,  nw.label.cex = 6,
network.plot = TRUE)
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = T,  nw.label.cex = 0.5,
network.plot = TRUE)
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = T,  nw.label.cex = 16,
network.plot = TRUE)
word_network_plot(text2, match.string = "мер",
stopwords = sw, proportional = T,  nw.label.cex = 16, label.size = 0.5)
word_network_plot(text2, target.words = "мер",
stopwords = sw, proportional = T,  nw.label.cex = 16, label.size = 0.5)
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = T,  nw.label.cex = 20,
network.plot = TRUE)
#Word network
sw <- c("и","в","во","не","что","он","на","я","например" , "ещё", "с","со","как","а","то","все","она","так","его","но","да","ты","к","у","же","вы","за","бы","по","только","ее","мне","было","вот","от","меня","еще","нет","о","из","ему","теперь","когда","даже","ну","вдруг","ли","если","уже","или","ни","быть","был","него","до","вас","нибудь","опять","уж","вам","сказал","ведь","там","потом","себя","ничего","ей","может","они","тут","где","есть","надо","ней","для","мы","тебя","их","чем","была","сам","чтоб","без","будто","человек","чего","раз","тоже","себе","под","жизнь","будет","ж","тогда","кто","этот","говорил","того","потому","этого","какой","совсем","ним","здесь","этом","один","почти","мой","тем","чтобы","нее","кажется","сейчас","были","куда","зачем","сказать","всех","никогда","сегодня","можно","при","наконец","два","об","другой","хоть","после","над","больше","тот","через","эти","нас","про","всего","них","какая","много","разве","сказала","три","эту","моя","впрочем","хорошо","свою","этой","перед","иногда","лучше","чуть","том","нельзя","такой","им","более","всегда","конечно","всю","между")
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = T,  nw.label.cex = 20,
network.plot = TRUE)
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = T,  nw.label.cex = 20,
network.plot = TRUE)
#Word network
sw <- c("и","в","во","не","что","он","на","я", "ещё", "с","со","как","а","то","все","она","так","его","но","да","ты","к","у","же","вы","за","бы","по","только","ее","мне","было","вот","от","меня","еще","нет","о","из","ему","теперь","когда","даже","ну","вдруг","ли","если","уже","или","ни","быть","был","него","до","вас","нибудь","опять","уж","вам","сказал","ведь","там","потом","себя","ничего","ей","может","они","тут","где","есть","надо","ней","для","мы","тебя","их","чем","была","сам","чтоб","без","будто","человек","чего","раз","тоже","себе","под","жизнь","будет","ж","тогда","кто","этот","говорил","того","потому","этого","какой","совсем","ним","здесь","этом","один","почти","мой","тем","чтобы","нее","кажется","сейчас","были","куда","зачем","сказать","всех","никогда","сегодня","можно","при","наконец","два","об","другой","хоть","после","над","больше","тот","через","эти","нас","про","всего","них","какая","много","разве","сказала","три","эту","моя","впрочем","хорошо","свою","этой","перед","иногда","лучше","чуть","том","нельзя","такой","им","более","всегда","конечно","всю","между")
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = T,  nw.label.cex = 20,
network.plot = TRUE)
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = T,  nw.label.cex = 16,
network.plot = TRUE)
#Word network
sw <- c("и","в","во","не","что","он","на","я", "людей","ещё", "с","со","как","а","то","все","она","так","его","но","да","ты","к","у","же","вы","за","бы","по","только","ее","мне","было","вот","от","меня","еще","нет","о","из","ему","теперь","когда","даже","ну","вдруг","ли","если","уже","или","ни","быть","был","него","до","вас","нибудь","опять","уж","вам","сказал","ведь","там","потом","себя","ничего","ей","может","они","тут","где","есть","надо","ней","для","мы","тебя","их","чем","была","сам","чтоб","без","будто","человек","чего","раз","тоже","себе","под","жизнь","будет","ж","тогда","кто","этот","говорил","того","потому","этого","какой","совсем","ним","здесь","этом","один","почти","мой","тем","чтобы","нее","кажется","сейчас","были","куда","зачем","сказать","всех","никогда","сегодня","можно","при","наконец","два","об","другой","хоть","после","над","больше","тот","через","эти","нас","про","всего","них","какая","много","разве","сказала","три","эту","моя","впрочем","хорошо","свою","этой","перед","иногда","лучше","чуть","том","нельзя","такой","им","более","всегда","конечно","всю","между")
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = T,  nw.label.cex = 16,
network.plot = TRUE)
#Word network
sw <- c("и","в","во","не","что","он","на","я", "федерации" , "людей","ещё", "с","со","как","а","то","все","она","так","его","но","да","ты","к","у","же","вы","за","бы","по","только","ее","мне","было","вот","от","меня","еще","нет","о","из","ему","теперь","когда","даже","ну","вдруг","ли","если","уже","или","ни","быть","был","него","до","вас","нибудь","опять","уж","вам","сказал","ведь","там","потом","себя","ничего","ей","может","они","тут","где","есть","надо","ней","для","мы","тебя","их","чем","была","сам","чтоб","без","будто","человек","чего","раз","тоже","себе","под","жизнь","будет","ж","тогда","кто","этот","говорил","того","потому","этого","какой","совсем","ним","здесь","этом","один","почти","мой","тем","чтобы","нее","кажется","сейчас","были","куда","зачем","сказать","всех","никогда","сегодня","можно","при","наконец","два","об","другой","хоть","после","над","больше","тот","через","эти","нас","про","всего","них","какая","много","разве","сказала","три","эту","моя","впрочем","хорошо","свою","этой","перед","иногда","лучше","чуть","том","нельзя","такой","им","более","всегда","конечно","всю","между")
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = T,  nw.label.cex = 16,
network.plot = TRUE)
#Word network
sw <- c("и","в","во","не","что","он","на","я", "ещё", "с","со","как","а","то","все","она","так","его","но","да","ты","к","у","же","вы","за","бы","по","только","ее","мне","было","вот","от","меня","еще","нет","о","из","ему","теперь","когда","даже","ну","вдруг","ли","если","уже","или","ни","быть","был","него","до","вас","нибудь","опять","уж","вам","сказал","ведь","там","потом","себя","ничего","ей","может","они","тут","где","есть","надо","ней","для","мы","тебя","их","чем","была","сам","чтоб","без","будто","человек","чего","раз","тоже","себе","под","жизнь","будет","ж","тогда","кто","этот","говорил","того","потому","этого","какой","совсем","ним","здесь","этом","один","почти","мой","тем","чтобы","нее","кажется","сейчас","были","куда","зачем","сказать","всех","никогда","сегодня","можно","при","наконец","два","об","другой","хоть","после","над","больше","тот","через","эти","нас","про","всего","них","какая","много","разве","сказала","три","эту","моя","впрочем","хорошо","свою","этой","перед","иногда","лучше","чуть","том","нельзя","такой","им","более","всегда","конечно","всю","между")
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = T,  nw.label.cex = 16,
network.plot = TRUE)
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = T,  nw.label.cex = 18,
network.plot = TRUE)
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = T,  nw.label.cex = 25,
network.plot = TRUE)
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = T,  nw.label.cex = 45,
network.plot = TRUE)
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = T,  nw.label.cex = 40,
network.plot = TRUE)
word_associate(text2, match.string = "мер",
stopwords = sw, proportional = T,  nw.label.cex = 20,
network.plot = TRUE)
# Packages
library(tm)
library(SnowballC)
library(wordcloud)
library(quanteda)
library(stm)
library(rio)
library(qdap)
library(tidyverse)
#Read data
text3 <- readLines("gubern08.04.txt", encoding = "UTF-8")
docs3 <- Corpus(VectorSource(text2))
inspect(docs3)
#Prepare data
docs3 <- tm_map(docs3, stripWhitespace)
docs3 <- tm_map(docs3, content_transformer(tolower))
docs3 <- tm_map(docs3, removePunctuation)
docs3 <- tm_map(docs3, removeWords, words = stopwords("russian"))
docs3 <- tm_map(docs3, removeWords, "или", "нам", "такж", "сво")
docs3 <- tm_map(docs3, stemDocument, language="russian")
#Text to term matrix
dtm3 <- TermDocumentMatrix(docs3)
m3 <- as.matrix(dtm3)
v3 <- sort(rowSums(m3),decreasing=TRUE)
d3 <- data.frame(word = names(v3),freq=v3)
head(d3, 23)
docs3 <- Corpus(VectorSource(text3))
inspect(docs3)
#Prepare data
docs3 <- tm_map(docs3, stripWhitespace)
docs3 <- tm_map(docs3, content_transformer(tolower))
docs3 <- tm_map(docs3, removePunctuation)
docs3 <- tm_map(docs3, removeWords, words = stopwords("russian"))
docs3 <- tm_map(docs3, removeWords, "или", "нам", "такж", "сво")
docs3 <- tm_map(docs3, stemDocument, language="russian")
#Text to term matrix
dtm3 <- TermDocumentMatrix(docs3)
m3 <- as.matrix(dtm3)
v3 <- sort(rowSums(m3),decreasing=TRUE)
d3 <- data.frame(word = names(v3),freq=v3)
head(d3, 23)
text3 <- readLines("gubern08.04.txt", encoding = "UTF-8")
docs3 <- Corpus(VectorSource(text3))
inspect(docs3)
#Prepare data
docs3 <- tm_map(docs3, stripWhitespace)
docs3 <- tm_map(docs3, content_transformer(tolower))
docs3 <- tm_map(docs3, removePunctuation)
docs3 <- tm_map(docs3, removeWords, words = stopwords("russian"))
docs3 <- tm_map(docs3, removeWords, "или", "нам", "такж", "сво")
docs3 <- tm_map(docs3, stemDocument, language="russian")
#Text to term matrix
dtm3 <- TermDocumentMatrix(docs3)
m3 <- as.matrix(dtm3)
v3 <- sort(rowSums(m3),decreasing=TRUE)
d3 <- data.frame(word = names(v3),freq=v3)
head(d3, 23)
docs3 <- tm_map(docs3, removeWords, "или", "нам", "кажд", "так", "такж", "сво", "эт", "котор")
docs3 <- tm_map(docs3, stemDocument, language="russian")
#Text to term matrix
dtm3 <- TermDocumentMatrix(docs3)
m3 <- as.matrix(dtm3)
v3 <- sort(rowSums(m3),decreasing=TRUE)
d3 <- data.frame(word = names(v3),freq=v3)
head(d3, 23)
text3 <- readLines("gubern08.04.txt", encoding = "UTF-8")
docs3 <- Corpus(VectorSource(text3))
inspect(docs3)
#Prepare data
docs3 <- tm_map(docs3, stripWhitespace)
docs3 <- tm_map(docs3, content_transformer(tolower))
docs3 <- tm_map(docs3, removePunctuation)
docs3 <- tm_map(docs3, removeWords, words = stopwords("russian"))
docs3 <- tm_map(docs3, removeWords, "или", "нам", "им", "кажд", "так", "такж", "сво", "эт", "котор")
docs3 <- tm_map(docs3, stemDocument, language="russian")
#Text to term matrix
dtm3 <- TermDocumentMatrix(docs3)
m3 <- as.matrix(dtm3)
v3 <- sort(rowSums(m3),decreasing=TRUE)
d3 <- data.frame(word = names(v3),freq=v3)
head(d3, 23)
removeWords
,removeWords
?removeWords
docs3 <- Corpus(VectorSource(text3))
inspect(docs3)
#Prepare data
docs3 <- tm_map(docs3, stripWhitespace)
docs3 <- tm_map(docs3, content_transformer(tolower))
docs3 <- tm_map(docs3, removePunctuation)
docs3 <- tm_map(docs3, removeWords, words = stopwords("russian"))
docs3 <- tm_map(docs3, removeWords, c("или", "нам", "им", "кажд", "так", "такж", "сво", "эт", "котор"))
docs3 <- tm_map(docs3, stemDocument, language="russian")
#Text to term matrix
dtm3 <- TermDocumentMatrix(docs3)
m3 <- as.matrix(dtm3)
v3 <- sort(rowSums(m3),decreasing=TRUE)
d3 <- data.frame(word = names(v3),freq=v3)
head(d3, 23)
docs3 <- tm_map(docs3, removeWords, words =  c("или", "нам", "им", "кажд", "так", "такж", "сво", "эт", "котор"))
docs3 <- tm_map(docs3, stemDocument, language="russian")
#Text to term matrix
dtm3 <- TermDocumentMatrix(docs3)
m3 <- as.matrix(dtm3)
v3 <- sort(rowSums(m3),decreasing=TRUE)
d3 <- data.frame(word = names(v3),freq=v3)
head(d3, 23)
View(d3)
#Frequency plot
ggplot(data = d3[1:18,], aes(x=fct_reorder(word,freq), y = freq, fill = as.factor(freq), col = as.factor(freq))) +
geom_bar(stat="identity") +
coord_flip() +
xlab("") +
ylab("Частота") +
theme(panel.background = element_rect(fill = "gray99"), axis.text.y = element_text(size=14),
axis.text.x = element_text(size=14), axis.title.x = element_text(size=14),
axis.title.y = element_text(size=14))
# Associations and cluster analysis
findAssocs(dtm2, c("работ", "регион", "сем", "рубл", "выплат",
"ситуац", "мусяц", "мер", "люд", "дополнительн"), 0.5)
# Associations and cluster analysis
findAssocs(dtm3, c("работ", "регион", "сем", "рубл", "выплат",
"ситуац", "мусяц", "мер", "люд", "дополнительн"), 0.5)
dm3 <- removeSparseTerms(dtm3, sparse=0.9)
d3 <- dist(as.matrix(dm3))
hc <- hclust(d3, method="ward.D")
plot(hc)
dm3 <- removeSparseTerms(dtm3, sparse=0.8)
d3 <- dist(as.matrix(dm3))
hc <- hclust(d3, method="ward.D")
plot(hc)
dm3 <- removeSparseTerms(dtm3, sparse=0.95)
d3 <- dist(as.matrix(dm3))
hc <- hclust(d3, method="ward.D")
plot(hc)
dm3 <- removeSparseTerms(dtm3, sparse=0.97)
d3 <- dist(as.matrix(dm3))
hc <- hclust(d3, method="ward.D")
plot(hc)
dm3 <- removeSparseTerms(dtm3, sparse=0.93)
d3 <- dist(as.matrix(dm3))
hc <- hclust(d3, method="ward.D")
plot(hc)
plot(hc)
groups <- cutree(hc, k=6)
rect.hclust(hc, k=6, border="red")
plot(hc)
groups <- cutree(hc, k=6)
rect.hclust(hc, k=6, border="red")
plot(hc)
groups <- cutree(hc, k=7)
rect.hclust(hc, k=7, border="red")
#Word network
sw <- c("и","в","во","не","что","он","на","я", "ещё", "с","со","как","а","то","все","она","так","его","но","да","ты","к","у","же","вы","за","бы","по","только","ее","мне","было","вот","от","меня","еще","нет","о","из","ему","теперь","когда","даже","ну","вдруг","ли","если","уже","или","ни","быть","был","него","до","вас","нибудь","опять","уж","вам","сказал","ведь","там","потом","себя","ничего","ей","может","они","тут","где","есть","надо","ней","для","мы","тебя","их","чем","была","сам","чтоб","без","будто","человек","чего","раз","тоже","себе","под","жизнь","будет","ж","тогда","кто","этот","говорил","того","потому","этого","какой","совсем","ним","здесь","этом","один","почти","мой","тем","чтобы","нее","кажется","сейчас","были","куда","зачем","сказать","всех","никогда","сегодня","можно","при","наконец","два","об","другой","хоть","после","над","больше","тот","через","эти","нас","про","всего","них","какая","много","разве","сказала","три","эту","моя","впрочем","хорошо","свою","этой","перед","иногда","лучше","чуть","том","нельзя","такой","им","более","всегда","конечно","всю","между")
word_associate(text3, match.string = "половц",
stopwords = sw, proportional = T,  nw.label.cex = 20,
network.plot = TRUE)
word_associate(text3, match.string = "печенеги",
stopwords = sw, proportional = T,  nw.label.cex = 20,
network.plot = TRUE)
word_associate(text3, match.string = "терзали",
stopwords = sw, proportional = T,  nw.label.cex = 20,
network.plot = TRUE)
word_associate(text3, match.string = "коронавирус",
stopwords = sw, proportional = T,  nw.label.cex = 20,
network.plot = TRUE)
#Word network
sw <- c("это","и","в","во","не","что","он","на","я", "ещё", "с","со","как","а","то","все","она","так","его","но","да","ты","к","у","же","вы","за","бы","по","только","ее","мне","было","вот","от","меня","еще","нет","о","из","ему","теперь","когда","даже","ну","вдруг","ли","если","уже","или","ни","быть","был","него","до","вас","нибудь","опять","уж","вам","сказал","ведь","там","потом","себя","ничего","ей","может","они","тут","где","есть","надо","ней","для","мы","тебя","их","чем","была","сам","чтоб","без","будто","человек","чего","раз","тоже","себе","под","жизнь","будет","ж","тогда","кто","этот","говорил","того","потому","этого","какой","совсем","ним","здесь","этом","один","почти","мой","тем","чтобы","нее","кажется","сейчас","были","куда","зачем","сказать","всех","никогда","сегодня","можно","при","наконец","два","об","другой","хоть","после","над","больше","тот","через","эти","нас","про","всего","них","какая","много","разве","сказала","три","эту","моя","впрочем","хорошо","свою","этой","перед","иногда","лучше","чуть","том","нельзя","такой","им","более","всегда","конечно","всю","между")
word_associate(text3, match.string = "коронавирус",
stopwords = sw, proportional = T,  nw.label.cex = 20,
network.plot = TRUE)
sw <- c("это","и","которые","в","во","не","что","он","на","я", "ещё", "с","со","как","а","то","все","она","так","его","но","да","ты","к","у","же","вы","за","бы","по","только","ее","мне","было","вот","от","меня","еще","нет","о","из","ему","теперь","когда","даже","ну","вдруг","ли","если","уже","или","ни","быть","был","него","до","вас","нибудь","опять","уж","вам","сказал","ведь","там","потом","себя","ничего","ей","может","они","тут","где","есть","надо","ней","для","мы","тебя","их","чем","была","сам","чтоб","без","будто","человек","чего","раз","тоже","себе","под","жизнь","будет","ж","тогда","кто","этот","говорил","того","потому","этого","какой","совсем","ним","здесь","этом","один","почти","мой","тем","чтобы","нее","кажется","сейчас","были","куда","зачем","сказать","всех","никогда","сегодня","можно","при","наконец","два","об","другой","хоть","после","над","больше","тот","через","эти","нас","про","всего","них","какая","много","разве","сказала","три","эту","моя","впрочем","хорошо","свою","этой","перед","иногда","лучше","чуть","том","нельзя","такой","им","более","всегда","конечно","всю","между")
word_associate(text3, match.string = "коронавирус",
stopwords = sw, proportional = T,  nw.label.cex = 20,
network.plot = TRUE)
sw <- c("это","и","которые","в","во","не","что","он","на","я", "ещё", "с","со","как","а","то","все","она","так","его","но","да","ты","к","у","же","вы","за","бы","по","только","ее","мне","было","вот","от","меня","еще","нет","о","из","ему","теперь","когда","даже","ну","вдруг","ли","если","уже","или","ни","быть","был","него","до","вас","нибудь","опять","уж","вам","сказал","ведь","там","потом","себя","ничего","ей","может","они","тут","где","есть","надо","ней","для","мы","тебя","их","чем","была","сам","чтоб","без","будто","человек","чего","раз","тоже","себе","под","жизнь","будет","ж","тогда","кто","этот","говорил","того","потому","этого","какой","совсем","ним","здесь","этом","один","почти","мой","тем","чтобы","нее","кажется","сейчас","были","куда","зачем","сказать","всех","никогда","сегодня","можно","при","наконец","два","об","другой","хоть","после","над","больше","тот","через","эти","нас","про","всего","них","какая","много","разве","сказала","три","эту","моя","впрочем","хорошо","свою","этой","перед","иногда","лучше","чуть","том","нельзя","такой","им","более","всегда","конечно","всю","между")
word_associate(text3, match.string = "коронавирус",
stopwords = c("russian", sw), proportional = T,  nw.label.cex = 20,
network.plot = TRUE)
word_associate(text3, match.string = "коронавирус",
stopwords = c("russian", sw), proportional = T,  nw.label.cex = 16,
network.plot = TRUE)
word_associate(text3, match.string = "'эпидеми",
stopwords = c("russian", sw), proportional = T,  nw.label.cex = 16,
network.plot = TRUE)
word_associate(text3, match.string = "эпидеми",
stopwords = c("russian", sw), proportional = T,  nw.label.cex = 16,
network.plot = TRUE)
word_associate(text3, match.string = "эпидеми",
stopwords = c("russian", sw), proportional = T,  nw.label.cex = 16,
network.plot = TRUE)
word_associate(text3, match.string = "эпидеми",
stopwords = c("russian", sw), proportional = T,  nw.label.cex = 26,
network.plot = TRUE)
# WordCloud
set.seed(4)
word_associate(text3, match.string = "эпидеми",
stopwords = c("russian", sw), proportional = T,  nw.label.cex = 26,
network.plot = TRUE)
set.seed(4)
wordcloud(words = d3$word, freq = d3$freq, min.freq=2, scale=c(2.2,0.1),
max.words=150, random.order=F, use.r.layout=FALSE,
colors=brewer.pal(8, "Dark2"))
#Read data
text3 <- readLines("gubern08.04.txt", encoding = "UTF-8")
docs3 <- Corpus(VectorSource(text3))
inspect(docs3)
#Prepare data
docs3 <- tm_map(docs3, stripWhitespace)
docs3 <- tm_map(docs3, content_transformer(tolower))
docs3 <- tm_map(docs3, removePunctuation)
docs3 <- tm_map(docs3, removeWords, words = stopwords("russian"))
docs3 <- tm_map(docs3, removeWords, words =  c("или", "нам", "им", "кажд", "так", "такж", "сво", "эт", "котор"))
docs3 <- tm_map(docs3, stemDocument, language="russian")
#Text to term matrix
dtm3 <- TermDocumentMatrix(docs3)
m3 <- as.matrix(dtm3)
v3 <- sort(rowSums(m3),decreasing=TRUE)
d3 <- data.frame(word = names(v3),freq=v3)
head(d3, 23)
wordcloud(words = d3$word, freq = d3$freq, min.freq=2, scale=c(2.2,0.1),
max.words=150, random.order=F, use.r.layout=FALSE,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d3$word, freq = d3$freq, min.freq=2, scale=c(2.2,0.1),
max.words=150, random.order=F, use.r.layout=FALSE,
colors=brewer.pal(8, "Dark2"))
docs3 <- tm_map(docs3, removeWords, words =  c("или", "нам", "им", "кажд", "так", "такж", "сво", "эт", "котор"))
#Text to term matrix
dtm3 <- TermDocumentMatrix(docs3)
m3 <- as.matrix(dtm3)
v3 <- sort(rowSums(m3),decreasing=TRUE)
d3 <- data.frame(word = names(v3),freq=v3)
head(d3, 23)
wordcloud(words = d3$word, freq = d3$freq, min.freq=2, scale=c(2.2,0.1),
max.words=150, random.order=F, use.r.layout=FALSE,
colors=brewer.pal(8, "Dark2"))
text3 <- readLines("gubern08.04.txt", encoding = "UTF-8")
docs3 <- Corpus(VectorSource(text3))
inspect(docs3)
#Prepare data
docs3 <- tm_map(docs3, stripWhitespace)
docs3 <- tm_map(docs3, content_transformer(tolower))
docs3 <- tm_map(docs3, removePunctuation)
docs3 <- tm_map(docs3, removeWords, words = stopwords("russian"))
docs3 <- tm_map(docs3, removeWords, words =  c("или", "нам", "им", "кажд", "так", "такж", "сво", "эт", "котор"))
dtm3 <- TermDocumentMatrix(docs3)
m3 <- as.matrix(dtm3)
v3 <- sort(rowSums(m3),decreasing=TRUE)
d3 <- data.frame(word = names(v3),freq=v3)
head(d3, 23)
set.seed(4)
wordcloud(words = d3$word, freq = d3$freq, min.freq=2, scale=c(2.2,0.1),
max.words=150, random.order=F, use.r.layout=FALSE,
colors=brewer.pal(8, "Dark2"))
word_associate(text3, match.string = "работ",
stopwords = c("russian", sw), proportional = T,  nw.label.cex = 26,
network.plot = TRUE)
# Associations and cluster analysis
findAssocs(dtm3, c("работ", "регион", "сем", "рубл", "выплат",
"ситуац", "месяц", "мер", "люд", "дополнительн"), 0.5)
# Associations and cluster analysis
findAssocs(dtm3, c("работ", "регион", "сем", "рубл", "выплат",
"ситуац", "месяц", "мер", "люд", "дополнительн"), 0.5)
text3 <- readLines("gubern08.04.txt", encoding = "UTF-8")
docs3 <- Corpus(VectorSource(text3))
inspect(docs3)
#Prepare data
docs3 <- tm_map(docs3, stripWhitespace)
docs3 <- tm_map(docs3, content_transformer(tolower))
docs3 <- tm_map(docs3, removePunctuation)
docs3 <- tm_map(docs3, removeWords, words = stopwords("russian"))
docs3 <- tm_map(docs3, removeWords, words =  c("или", "нам", "им", "кажд", "так", "такж", "сво", "эт", "котор"))
docs3 <- tm_map(docs3, stemDocument, language="russian")
#Text to term matrix
dtm3 <- TermDocumentMatrix(docs3)
m3 <- as.matrix(dtm3)
v3 <- sort(rowSums(m3),decreasing=TRUE)
d3 <- data.frame(word = names(v3),freq=v3)
head(d3, 23)
# Associations and cluster analysis
findAssocs(dtm3, c("работ", "регион", "сем", "рубл", "выплат",
"ситуац", "месяц", "мер", "люд", "дополнительн"), 0.5)
